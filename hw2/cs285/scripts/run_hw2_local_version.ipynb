{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd3a8d2",
   "metadata": {},
   "source": [
    "# Goal\n",
    "create a notebook version of run_hw2.py to learn the details of every step\n",
    "\n",
    "**questions**\n",
    "1. (theoretical) why reward-to-go's discount factor is adjusted to the current time period: reward_to_go[-1] = rewards[-1] rather than rewards[-1]*gamma**T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9fa709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/resource_handle_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/resource_handle_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/resource_handle_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/summary_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/summary_pb2.py:38: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/proto/summary_pb2.py:31: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "from cs285.agents.pg_agent import PGAgent\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "\n",
    "from cs285.infrastructure import utils\n",
    "from cs285.infrastructure.logger import Logger\n",
    "from cs285.infrastructure.action_noise_wrapper import ActionNoiseWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77704644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "----------------------------------------\n",
      "env_name                 : CartPole-v0\n",
      "exp_name                 : cartpole_baseline\n",
      "n_iter                   : 100\n",
      "use_reward_to_go         : True\n",
      "use_baseline             : True\n",
      "baseline_learning_rate   : 0.005\n",
      "baseline_gradient_steps  : 5\n",
      "gae_lambda               : None\n",
      "normalize_advantages     : True\n",
      "batch_size               : 1000\n",
      "eval_batch_size          : 400\n",
      "discount                 : 1.0\n",
      "learning_rate            : 0.005\n",
      "n_layers                 : 2\n",
      "layer_size               : 64\n",
      "ep_len                   : None\n",
      "seed                     : 1\n",
      "no_gpu                   : False\n",
      "which_gpu                : 0\n",
      "video_log_freq           : -1\n",
      "scalar_log_freq          : 1\n",
      "action_noise_std         : 0\n",
      "logdir                   : /Users/xing.zhang/machine-learning/homework_fall2023/hw2/cs285/scripts/data/q2_pg_cartpole_baseline_CartPole-v0_01-11-2025_19-41-07\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fixed TrainingArgs class that works in Jupyter notebooks\n",
    "@dataclass\n",
    "class TrainingArgs:\n",
    "    \"\"\"Configuration class for training arguments that works well in Jupyter notebooks.\"\"\"\n",
    "    \n",
    "    # Required arguments\n",
    "    env_name: str\n",
    "    exp_name: str\n",
    "    \n",
    "    # Training parameters\n",
    "    n_iter: int = 10\n",
    "    \n",
    "    # Policy gradient specific\n",
    "    use_reward_to_go: bool = False\n",
    "    use_baseline: bool = False\n",
    "    baseline_learning_rate: float = 5e-3\n",
    "    baseline_gradient_steps: int = 5\n",
    "    gae_lambda: Optional[float] = None\n",
    "    normalize_advantages: bool = False\n",
    "    \n",
    "    # Batch sizes\n",
    "    batch_size: int = 1000  # steps collected per train iteration\n",
    "    eval_batch_size: int = 400  # steps collected per eval iteration\n",
    "    \n",
    "    # Network parameters\n",
    "    discount: float = 1.0\n",
    "    learning_rate: float = 5e-3\n",
    "    n_layers: int = 2\n",
    "    layer_size: int = 64\n",
    "    \n",
    "    # Environment and logging\n",
    "    ep_len: Optional[int] = None  # students shouldn't change this away from env's default\n",
    "    seed: int = 1\n",
    "    no_gpu: bool = False\n",
    "    which_gpu: int = 0\n",
    "    video_log_freq: int = -1\n",
    "    scalar_log_freq: int = 1\n",
    "    action_noise_std: float = 0\n",
    "    \n",
    "    # Computed properties\n",
    "    logdir: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set up logging directory after initialization.\"\"\"\n",
    "        if self.logdir is None:\n",
    "            self._setup_logdir()\n",
    "    \n",
    "    def _setup_logdir(self):\n",
    "        \"\"\"Create and set up the logging directory.\"\"\"\n",
    "        logdir_prefix = \"q2_pg_\"  # keep for autograder\n",
    "        \n",
    "        # For notebooks, use current working directory instead of __file__\n",
    "        try:\n",
    "            # Try to get the script directory if running from a script\n",
    "            script_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "            data_path = os.path.join(script_dir, \"../../data\")\n",
    "        except NameError:\n",
    "            # If __file__ is not defined (like in notebooks), use current directory\n",
    "            data_path = os.path.join(os.getcwd(), \"data\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            os.makedirs(data_path)\n",
    "        \n",
    "        logdir = (\n",
    "            logdir_prefix\n",
    "            + self.exp_name\n",
    "            + \"_\"\n",
    "            + self.env_name\n",
    "            + \"_\"\n",
    "            + time.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "        )\n",
    "        self.logdir = os.path.join(data_path, logdir)\n",
    "        \n",
    "        if not os.path.exists(self.logdir):\n",
    "            os.makedirs(self.logdir)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: dict) -> 'TrainingArgs':\n",
    "        \"\"\"Create TrainingArgs from a dictionary (useful for notebook cells).\"\"\"\n",
    "        return cls(**config_dict)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert TrainingArgs to dictionary.\"\"\"\n",
    "        return {\n",
    "            field.name: getattr(self, field.name) \n",
    "            for field in self.__dataclass_fields__.values()\n",
    "        }\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        \"\"\"Update specific arguments.\"\"\"\n",
    "        for key, value in kwargs.items():\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, value)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown argument: {key}\")\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Print the current configuration.\"\"\"\n",
    "        print(\"Training Configuration:\")\n",
    "        print(\"-\" * 40)\n",
    "        for field in self.__dataclass_fields__:\n",
    "            value = getattr(self, field)\n",
    "            print(f\"{field:25}: {value}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Predefined configurations for common experiments\n",
    "def get_cartpole_config() -> TrainingArgs:\n",
    "    \"\"\"Get a configuration for CartPole experiments.\"\"\"\n",
    "    return TrainingArgs(\n",
    "        env_name=\"CartPole-v0\",\n",
    "        exp_name=\"cartpole_baseline\",\n",
    "        n_iter=100,\n",
    "        batch_size=1000,\n",
    "        eval_batch_size=400,\n",
    "        learning_rate=5e-3,\n",
    "        use_baseline=True,\n",
    "        use_reward_to_go=True,\n",
    "        normalize_advantages=True\n",
    "    )\n",
    "\n",
    "# Create an instance\n",
    "args = get_cartpole_config()\n",
    "args.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d84b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /Users/xing.zhang/machine-learning/homework_fall2023/hw2/cs285/scripts/data/q2_pg_cartpole_baseline_CartPole-v0_01-11-2025_19-41-07\n",
      "########################\n",
      "Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "MAX_NVIDEO = 2\n",
    "\n",
    "logger = Logger(args.logdir)\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "ptu.init_gpu(use_gpu=not args.no_gpu, gpu_id=args.which_gpu)\n",
    "\n",
    "# make the gym environment\n",
    "env = gym.make(args.env_name, render_mode=None)\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# add action noise, if needed\n",
    "if args.action_noise_std > 0:\n",
    "    assert not discrete, f\"Cannot use --action_noise_std for discrete environment {args.env_name}\"\n",
    "    env = ActionNoiseWrapper(env, args.seed, args.action_noise_std)\n",
    "\n",
    "max_ep_len = args.ep_len or env.spec.max_episode_steps\n",
    "\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# simulation timestep, will be used for video saving\n",
    "if hasattr(env, \"model\"):\n",
    "    fps = 1 / env.model.opt.timestep\n",
    "else:\n",
    "    fps = env.env.metadata[\"render_fps\"]\n",
    "\n",
    "from cs285.networks.policies import MLPPolicyPG\n",
    "actor = MLPPolicyPG(ac_dim, ob_dim, discrete, args.n_layers, args.layer_size, args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e303a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trajs\n",
      "[{'observation': array([[ 0.00742216,  0.01336038,  0.01530321, -0.02889315],\n",
      "       [ 0.00768937,  0.20825957,  0.01472535, -0.3167087 ],\n",
      "       [ 0.01185456,  0.4031687 ,  0.00839117, -0.60471165]],\n",
      "      dtype=float32), 'image_obs': array([], dtype=uint8), 'reward': array([1., 1., 1.], dtype=float32), 'action': array([1., 1., 1.], dtype=float32), 'next_observation': array([[ 0.00768937,  0.20825957,  0.01472535, -0.3167087 ],\n",
      "       [ 0.01185456,  0.4031687 ,  0.00839117, -0.60471165],\n",
      "       [ 0.01991793,  0.5981723 , -0.00370306, -0.8947398 ]],\n",
      "      dtype=float32), 'terminal': array([0., 0., 1.], dtype=float32)}, {'observation': array([[-0.00694073,  0.00904685,  0.04906083, -0.03109459],\n",
      "       [-0.00675979,  0.20343216,  0.04843893, -0.30790362],\n",
      "       [-0.00269115,  0.00765468,  0.04228086, -0.00034637]],\n",
      "      dtype=float32), 'image_obs': array([], dtype=uint8), 'reward': array([1., 1., 1.], dtype=float32), 'action': array([1., 0., 0.], dtype=float32), 'next_observation': array([[-0.00675979,  0.20343216,  0.04843893, -0.30790362],\n",
      "       [-0.00269115,  0.00765468,  0.04228086, -0.00034637],\n",
      "       [-0.00253805, -0.18804733,  0.04227393,  0.30537117]],\n",
      "      dtype=float32), 'terminal': array([0., 0., 1.], dtype=float32)}, {'observation': array([[-0.02821882,  0.0490177 ,  0.00132561, -0.00279631],\n",
      "       [-0.02723847,  0.24412061,  0.00126968, -0.2950607 ],\n",
      "       [-0.02235606,  0.04898058, -0.00463153, -0.0019776 ]],\n",
      "      dtype=float32), 'image_obs': array([], dtype=uint8), 'reward': array([1., 1., 1.], dtype=float32), 'action': array([1., 0., 0.], dtype=float32), 'next_observation': array([[-0.02723847,  0.24412061,  0.00126968, -0.2950607 ],\n",
      "       [-0.02235606,  0.04898058, -0.00463153, -0.0019776 ],\n",
      "       [-0.02137645, -0.14607464, -0.00467108,  0.28924042]],\n",
      "      dtype=float32), 'terminal': array([0., 0., 1.], dtype=float32)}, {'observation': array([[ 0.01126755,  0.00075667, -0.00228935,  0.012199  ],\n",
      "       [ 0.01128269,  0.19591138, -0.00204537, -0.28120536],\n",
      "       [ 0.01520092,  0.39106244, -0.00766948, -0.5745327 ]],\n",
      "      dtype=float32), 'image_obs': array([], dtype=uint8), 'reward': array([1., 1., 1.], dtype=float32), 'action': array([1., 1., 0.], dtype=float32), 'next_observation': array([[ 0.01128269,  0.19591138, -0.00204537, -0.28120536],\n",
      "       [ 0.01520092,  0.39106244, -0.00766948, -0.5745327 ],\n",
      "       [ 0.02302217,  0.19604886, -0.01916013, -0.2842757 ]],\n",
      "      dtype=float32), 'terminal': array([0., 0., 1.], dtype=float32)}]\n",
      "\n",
      "envstpes this batch\n",
      "12\n",
      "\n",
      "trajs_dict\n",
      "{'observation': [array([[ 0.00742216,  0.01336038,  0.01530321, -0.02889315],\n",
      "       [ 0.00768937,  0.20825957,  0.01472535, -0.3167087 ],\n",
      "       [ 0.01185456,  0.4031687 ,  0.00839117, -0.60471165]],\n",
      "      dtype=float32), array([[-0.00694073,  0.00904685,  0.04906083, -0.03109459],\n",
      "       [-0.00675979,  0.20343216,  0.04843893, -0.30790362],\n",
      "       [-0.00269115,  0.00765468,  0.04228086, -0.00034637]],\n",
      "      dtype=float32), array([[-0.02821882,  0.0490177 ,  0.00132561, -0.00279631],\n",
      "       [-0.02723847,  0.24412061,  0.00126968, -0.2950607 ],\n",
      "       [-0.02235606,  0.04898058, -0.00463153, -0.0019776 ]],\n",
      "      dtype=float32), array([[ 0.01126755,  0.00075667, -0.00228935,  0.012199  ],\n",
      "       [ 0.01128269,  0.19591138, -0.00204537, -0.28120536],\n",
      "       [ 0.01520092,  0.39106244, -0.00766948, -0.5745327 ]],\n",
      "      dtype=float32)], 'image_obs': [array([], dtype=uint8), array([], dtype=uint8), array([], dtype=uint8), array([], dtype=uint8)], 'reward': [array([1., 1., 1.], dtype=float32), array([1., 1., 1.], dtype=float32), array([1., 1., 1.], dtype=float32), array([1., 1., 1.], dtype=float32)], 'action': [array([1., 1., 1.], dtype=float32), array([1., 0., 0.], dtype=float32), array([1., 0., 0.], dtype=float32), array([1., 1., 0.], dtype=float32)], 'next_observation': [array([[ 0.00768937,  0.20825957,  0.01472535, -0.3167087 ],\n",
      "       [ 0.01185456,  0.4031687 ,  0.00839117, -0.60471165],\n",
      "       [ 0.01991793,  0.5981723 , -0.00370306, -0.8947398 ]],\n",
      "      dtype=float32), array([[-0.00675979,  0.20343216,  0.04843893, -0.30790362],\n",
      "       [-0.00269115,  0.00765468,  0.04228086, -0.00034637],\n",
      "       [-0.00253805, -0.18804733,  0.04227393,  0.30537117]],\n",
      "      dtype=float32), array([[-0.02723847,  0.24412061,  0.00126968, -0.2950607 ],\n",
      "       [-0.02235606,  0.04898058, -0.00463153, -0.0019776 ],\n",
      "       [-0.02137645, -0.14607464, -0.00467108,  0.28924042]],\n",
      "      dtype=float32), array([[ 0.01128269,  0.19591138, -0.00204537, -0.28120536],\n",
      "       [ 0.01520092,  0.39106244, -0.00766948, -0.5745327 ],\n",
      "       [ 0.02302217,  0.19604886, -0.01916013, -0.2842757 ]],\n",
      "      dtype=float32)], 'terminal': [array([0., 0., 1.], dtype=float32), array([0., 0., 1.], dtype=float32), array([0., 0., 1.], dtype=float32), array([0., 0., 1.], dtype=float32)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# two key factors: data (trajs) and agent\n",
    "# -----------------------------------------\n",
    "# 1. data (trajs): a list of dictionary for each episode\n",
    "example_batch_size = 10\n",
    "example_ep_len = 3\n",
    "trajs, envsteps_this_batch = utils.sample_trajectories(env, actor, example_batch_size, example_ep_len)\n",
    "print(\"\\ntrajs\\n\", trajs, sep = '')\n",
    "print(\"\\nenvstpes this batch\\n\", envsteps_this_batch, sep = '')\n",
    "# put each key together\n",
    "trajs_dict = {k: [traj[k] for traj in trajs] for k in trajs[0]}\n",
    "\n",
    "print(\"\\ntrajs_dict\\n\", trajs_dict, sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2907feed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 21.105262756347656\n",
      "Eval_StdReturn : 7.690696716308594\n",
      "Eval_MaxReturn : 36.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 21.105263157894736\n",
      "Train_AverageReturn : 23.952381134033203\n",
      "Train_StdReturn : 14.362570762634277\n",
      "Train_MaxReturn : 74.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 23.952380952380953\n",
      "Actor Loss : 0.0015725807752460241\n",
      "Baseline Loss : 445.3271484375\n",
      "Train_EnvstepsSoFar : 1006\n",
      "TimeSinceStart : 0.09609627723693848\n",
      "Initial_DataCollection_AverageReturn : 23.952381134033203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data for eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.10/site-packages/tensorboardX/summary.py:153: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  scalar = float(scalar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval_AverageReturn : 38.09090805053711\n",
      "Eval_StdReturn : 18.951873779296875\n",
      "Eval_MaxReturn : 75.0\n",
      "Eval_MinReturn : 11.0\n",
      "Eval_AverageEpLen : 38.09090909090909\n",
      "Train_AverageReturn : 27.2702693939209\n",
      "Train_StdReturn : 15.356393814086914\n",
      "Train_MaxReturn : 73.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.27027027027027\n",
      "Actor Loss : -0.003947851713746786\n",
      "Baseline Loss : 449.7375793457031\n",
      "Train_EnvstepsSoFar : 2015\n",
      "TimeSinceStart : 0.18341517448425293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 33.41666793823242\n",
      "Eval_StdReturn : 18.459226608276367\n",
      "Eval_MaxReturn : 86.0\n",
      "Eval_MinReturn : 13.0\n",
      "Eval_AverageEpLen : 33.416666666666664\n",
      "Train_AverageReturn : 30.81818199157715\n",
      "Train_StdReturn : 17.31255340576172\n",
      "Train_MaxReturn : 91.0\n",
      "Train_MinReturn : 13.0\n",
      "Train_AverageEpLen : 30.818181818181817\n",
      "Actor Loss : -0.005495672579854727\n",
      "Baseline Loss : 556.9501953125\n",
      "Train_EnvstepsSoFar : 3032\n",
      "TimeSinceStart : 0.26929521560668945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 50.625\n",
      "Eval_StdReturn : 24.248388290405273\n",
      "Eval_MaxReturn : 90.0\n",
      "Eval_MinReturn : 16.0\n",
      "Eval_AverageEpLen : 50.625\n",
      "Train_AverageReturn : 35.46666717529297\n",
      "Train_StdReturn : 20.97734260559082\n",
      "Train_MaxReturn : 91.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 35.46666666666667\n",
      "Actor Loss : -0.010286279022693634\n",
      "Baseline Loss : 649.5863647460938\n",
      "Train_EnvstepsSoFar : 4096\n",
      "TimeSinceStart : 0.35626912117004395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 48.88888931274414\n",
      "Eval_StdReturn : 28.03745460510254\n",
      "Eval_MaxReturn : 120.0\n",
      "Eval_MinReturn : 21.0\n",
      "Eval_AverageEpLen : 48.888888888888886\n",
      "Train_AverageReturn : 48.238094329833984\n",
      "Train_StdReturn : 25.069246292114258\n",
      "Train_MaxReturn : 113.0\n",
      "Train_MinReturn : 20.0\n",
      "Train_AverageEpLen : 48.23809523809524\n",
      "Actor Loss : -0.004220306873321533\n",
      "Baseline Loss : 1052.4583740234375\n",
      "Train_EnvstepsSoFar : 5109\n",
      "TimeSinceStart : 0.4443991184234619\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 58.57143020629883\n",
      "Eval_StdReturn : 27.937978744506836\n",
      "Eval_MaxReturn : 106.0\n",
      "Eval_MinReturn : 21.0\n",
      "Eval_AverageEpLen : 58.57142857142857\n",
      "Train_AverageReturn : 48.619049072265625\n",
      "Train_StdReturn : 25.793088912963867\n",
      "Train_MaxReturn : 102.0\n",
      "Train_MinReturn : 12.0\n",
      "Train_AverageEpLen : 48.61904761904762\n",
      "Actor Loss : -0.012668071314692497\n",
      "Baseline Loss : 968.06201171875\n",
      "Train_EnvstepsSoFar : 6130\n",
      "TimeSinceStart : 0.5342731475830078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 59.85714340209961\n",
      "Eval_StdReturn : 21.216089248657227\n",
      "Eval_MaxReturn : 94.0\n",
      "Eval_MinReturn : 30.0\n",
      "Eval_AverageEpLen : 59.857142857142854\n",
      "Train_AverageReturn : 53.21052551269531\n",
      "Train_StdReturn : 30.62778091430664\n",
      "Train_MaxReturn : 155.0\n",
      "Train_MinReturn : 23.0\n",
      "Train_AverageEpLen : 53.21052631578947\n",
      "Actor Loss : -0.010075809434056282\n",
      "Baseline Loss : 1456.7630615234375\n",
      "Train_EnvstepsSoFar : 7141\n",
      "TimeSinceStart : 0.6224651336669922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 81.4000015258789\n",
      "Eval_StdReturn : 45.12028503417969\n",
      "Eval_MaxReturn : 148.0\n",
      "Eval_MinReturn : 22.0\n",
      "Eval_AverageEpLen : 81.4\n",
      "Train_AverageReturn : 62.6875\n",
      "Train_StdReturn : 25.619131088256836\n",
      "Train_MaxReturn : 135.0\n",
      "Train_MinReturn : 32.0\n",
      "Train_AverageEpLen : 62.6875\n",
      "Actor Loss : -0.013728369027376175\n",
      "Baseline Loss : 1211.1195068359375\n",
      "Train_EnvstepsSoFar : 8144\n",
      "TimeSinceStart : 0.7063469886779785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 66.66666412353516\n",
      "Eval_StdReturn : 21.56900978088379\n",
      "Eval_MaxReturn : 110.0\n",
      "Eval_MinReturn : 44.0\n",
      "Eval_AverageEpLen : 66.66666666666667\n",
      "Train_AverageReturn : 64.375\n",
      "Train_StdReturn : 41.11397933959961\n",
      "Train_MaxReturn : 188.0\n",
      "Train_MinReturn : 27.0\n",
      "Train_AverageEpLen : 64.375\n",
      "Actor Loss : 0.0001800300960894674\n",
      "Baseline Loss : 2427.882568359375\n",
      "Train_EnvstepsSoFar : 9174\n",
      "TimeSinceStart : 0.792046070098877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 101.25\n",
      "Eval_StdReturn : 61.348899841308594\n",
      "Eval_MaxReturn : 199.0\n",
      "Eval_MinReturn : 30.0\n",
      "Eval_AverageEpLen : 101.25\n",
      "Train_AverageReturn : 70.33333587646484\n",
      "Train_StdReturn : 26.418848037719727\n",
      "Train_MaxReturn : 118.0\n",
      "Train_MinReturn : 33.0\n",
      "Train_AverageEpLen : 70.33333333333333\n",
      "Actor Loss : -0.012288498692214489\n",
      "Baseline Loss : 1199.622802734375\n",
      "Train_EnvstepsSoFar : 10229\n",
      "TimeSinceStart : 0.8793742656707764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 130.5\n",
      "Eval_StdReturn : 31.815876007080078\n",
      "Eval_MaxReturn : 185.0\n",
      "Eval_MinReturn : 105.0\n",
      "Eval_AverageEpLen : 130.5\n",
      "Train_AverageReturn : 88.83333587646484\n",
      "Train_StdReturn : 33.01472854614258\n",
      "Train_MaxReturn : 147.0\n",
      "Train_MinReturn : 29.0\n",
      "Train_AverageEpLen : 88.83333333333333\n",
      "Actor Loss : -0.011722703464329243\n",
      "Baseline Loss : 2069.889892578125\n",
      "Train_EnvstepsSoFar : 11295\n",
      "TimeSinceStart : 0.9732582569122314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 82.4000015258789\n",
      "Eval_StdReturn : 7.0597453117370605\n",
      "Eval_MaxReturn : 92.0\n",
      "Eval_MinReturn : 74.0\n",
      "Eval_AverageEpLen : 82.4\n",
      "Train_AverageReturn : 113.33333587646484\n",
      "Train_StdReturn : 55.190582275390625\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 26.0\n",
      "Train_AverageEpLen : 113.33333333333333\n",
      "Actor Loss : -0.0007938609342090786\n",
      "Baseline Loss : 4731.65234375\n",
      "Train_EnvstepsSoFar : 12315\n",
      "TimeSinceStart : 1.0583412647247314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.6666717529297\n",
      "Eval_StdReturn : 10.208929061889648\n",
      "Eval_MaxReturn : 175.0\n",
      "Eval_MinReturn : 150.0\n",
      "Eval_AverageEpLen : 162.66666666666666\n",
      "Train_AverageReturn : 101.80000305175781\n",
      "Train_StdReturn : 43.57935333251953\n",
      "Train_MaxReturn : 183.0\n",
      "Train_MinReturn : 37.0\n",
      "Train_AverageEpLen : 101.8\n",
      "Actor Loss : -0.011207512579858303\n",
      "Baseline Loss : 3060.024169921875\n",
      "Train_EnvstepsSoFar : 13333\n",
      "TimeSinceStart : 1.1465201377868652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.0\n",
      "Eval_StdReturn : 40.13311004638672\n",
      "Eval_MaxReturn : 191.0\n",
      "Eval_MinReturn : 99.0\n",
      "Eval_AverageEpLen : 135.0\n",
      "Train_AverageReturn : 91.90908813476562\n",
      "Train_StdReturn : 44.57160568237305\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 27.0\n",
      "Train_AverageEpLen : 91.9090909090909\n",
      "Actor Loss : -0.0005840756348334253\n",
      "Baseline Loss : 2720.43505859375\n",
      "Train_EnvstepsSoFar : 14344\n",
      "TimeSinceStart : 1.230149269104004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 170.6666717529297\n",
      "Eval_StdReturn : 35.31131362915039\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 121.0\n",
      "Eval_AverageEpLen : 170.66666666666666\n",
      "Train_AverageReturn : 149.625\n",
      "Train_StdReturn : 28.561063766479492\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 102.0\n",
      "Train_AverageEpLen : 149.625\n",
      "Actor Loss : 0.0011305151274427772\n",
      "Baseline Loss : 4507.18017578125\n",
      "Train_EnvstepsSoFar : 15541\n",
      "TimeSinceStart : 1.3288121223449707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 197.6666717529297\n",
      "Eval_StdReturn : 3.2998316287994385\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 193.0\n",
      "Eval_AverageEpLen : 197.66666666666666\n",
      "Train_AverageReturn : 164.14285278320312\n",
      "Train_StdReturn : 34.26963806152344\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 106.0\n",
      "Train_AverageEpLen : 164.14285714285714\n",
      "Actor Loss : -0.012869659811258316\n",
      "Baseline Loss : 5582.65087890625\n",
      "Train_EnvstepsSoFar : 16690\n",
      "TimeSinceStart : 1.429718017578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 189.3333282470703\n",
      "Eval_StdReturn : 6.6499786376953125\n",
      "Eval_MaxReturn : 195.0\n",
      "Eval_MinReturn : 180.0\n",
      "Eval_AverageEpLen : 189.33333333333334\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.012659045867621899\n",
      "Baseline Loss : 7538.77490234375\n",
      "Train_EnvstepsSoFar : 17690\n",
      "TimeSinceStart : 1.5218892097473145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 199.1666717529297\n",
      "Train_StdReturn : 1.8633898496627808\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 195.0\n",
      "Train_AverageEpLen : 199.16666666666666\n",
      "Actor Loss : -0.02564103715121746\n",
      "Baseline Loss : 7149.0830078125\n",
      "Train_EnvstepsSoFar : 18885\n",
      "TimeSinceStart : 1.6151201725006104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.006640635896474123\n",
      "Baseline Loss : 6906.2578125\n",
      "Train_EnvstepsSoFar : 19885\n",
      "TimeSinceStart : 1.701404333114624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 169.8333282470703\n",
      "Train_StdReturn : 38.774635314941406\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 100.0\n",
      "Train_AverageEpLen : 169.83333333333334\n",
      "Actor Loss : -0.011556001380085945\n",
      "Baseline Loss : 5221.58837890625\n",
      "Train_EnvstepsSoFar : 20904\n",
      "TimeSinceStart : 1.7890472412109375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.006809069309383631\n",
      "Baseline Loss : 6362.3037109375\n",
      "Train_EnvstepsSoFar : 21904\n",
      "TimeSinceStart : 1.8746590614318848\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.007127744611352682\n",
      "Baseline Loss : 6090.84423828125\n",
      "Train_EnvstepsSoFar : 22904\n",
      "TimeSinceStart : 1.9595232009887695\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 195.1666717529297\n",
      "Train_StdReturn : 10.807662010192871\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 171.0\n",
      "Train_AverageEpLen : 195.16666666666666\n",
      "Actor Loss : -0.0009971247054636478\n",
      "Baseline Loss : 5496.7607421875\n",
      "Train_EnvstepsSoFar : 24075\n",
      "TimeSinceStart : 2.0524511337280273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 198.3333282470703\n",
      "Train_StdReturn : 3.7267799377441406\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 190.0\n",
      "Train_AverageEpLen : 198.33333333333334\n",
      "Actor Loss : -0.008213200606405735\n",
      "Baseline Loss : 5475.505859375\n",
      "Train_EnvstepsSoFar : 25265\n",
      "TimeSinceStart : 2.1457972526550293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.002899553393945098\n",
      "Baseline Loss : 5317.90478515625\n",
      "Train_EnvstepsSoFar : 26265\n",
      "TimeSinceStart : 2.230525255203247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.01808279938995838\n",
      "Baseline Loss : 5070.12158203125\n",
      "Train_EnvstepsSoFar : 27265\n",
      "TimeSinceStart : 2.3182363510131836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.008324498310685158\n",
      "Baseline Loss : 5083.1865234375\n",
      "Train_EnvstepsSoFar : 28265\n",
      "TimeSinceStart : 2.4052023887634277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.023462358862161636\n",
      "Baseline Loss : 4840.81591796875\n",
      "Train_EnvstepsSoFar : 29265\n",
      "TimeSinceStart : 2.490626096725464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.005909412167966366\n",
      "Baseline Loss : 4678.2060546875\n",
      "Train_EnvstepsSoFar : 30265\n",
      "TimeSinceStart : 2.5728821754455566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.005317854695022106\n",
      "Baseline Loss : 4526.63623046875\n",
      "Train_EnvstepsSoFar : 31265\n",
      "TimeSinceStart : 2.6560471057891846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.02216675691306591\n",
      "Baseline Loss : 4508.47314453125\n",
      "Train_EnvstepsSoFar : 32265\n",
      "TimeSinceStart : 2.739234209060669\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.003698024433106184\n",
      "Baseline Loss : 4463.63427734375\n",
      "Train_EnvstepsSoFar : 33265\n",
      "TimeSinceStart : 2.822230100631714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.013756835833191872\n",
      "Baseline Loss : 4385.6259765625\n",
      "Train_EnvstepsSoFar : 34265\n",
      "TimeSinceStart : 2.9063751697540283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.003811146365478635\n",
      "Baseline Loss : 4301.87255859375\n",
      "Train_EnvstepsSoFar : 35265\n",
      "TimeSinceStart : 2.993157148361206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 179.6666717529297\n",
      "Eval_StdReturn : 25.978622436523438\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 143.0\n",
      "Eval_AverageEpLen : 179.66666666666666\n",
      "Train_AverageReturn : 191.8333282470703\n",
      "Train_StdReturn : 18.26122283935547\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 151.0\n",
      "Train_AverageEpLen : 191.83333333333334\n",
      "Actor Loss : -0.008370223455131054\n",
      "Baseline Loss : 3916.337890625\n",
      "Train_EnvstepsSoFar : 36416\n",
      "TimeSinceStart : 3.098670244216919\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.009445445612072945\n",
      "Baseline Loss : 4145.86767578125\n",
      "Train_EnvstepsSoFar : 37416\n",
      "TimeSinceStart : 3.185671329498291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 175.6666717529297\n",
      "Eval_StdReturn : 34.41252899169922\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 127.0\n",
      "Eval_AverageEpLen : 175.66666666666666\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.012773086316883564\n",
      "Baseline Loss : 4079.57861328125\n",
      "Train_EnvstepsSoFar : 38416\n",
      "TimeSinceStart : 3.2772281169891357\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 164.3333282470703\n",
      "Eval_StdReturn : 32.70405960083008\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 121.0\n",
      "Eval_AverageEpLen : 164.33333333333334\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.014362698420882225\n",
      "Baseline Loss : 4017.87841796875\n",
      "Train_EnvstepsSoFar : 39416\n",
      "TimeSinceStart : 3.3670053482055664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.6666717529297\n",
      "Eval_StdReturn : 32.05550765991211\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 132.0\n",
      "Eval_AverageEpLen : 154.66666666666666\n",
      "Train_AverageReturn : 189.3333282470703\n",
      "Train_StdReturn : 23.851390838623047\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 136.0\n",
      "Train_AverageEpLen : 189.33333333333334\n",
      "Actor Loss : -0.019963569939136505\n",
      "Baseline Loss : 3677.89306640625\n",
      "Train_EnvstepsSoFar : 40552\n",
      "TimeSinceStart : 3.4622273445129395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.3333282470703\n",
      "Eval_StdReturn : 57.5113525390625\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 78.0\n",
      "Eval_AverageEpLen : 159.33333333333334\n",
      "Train_AverageReturn : 167.8333282470703\n",
      "Train_StdReturn : 44.607234954833984\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 87.0\n",
      "Train_AverageEpLen : 167.83333333333334\n",
      "Actor Loss : -0.014378654770553112\n",
      "Baseline Loss : 3343.18408203125\n",
      "Train_EnvstepsSoFar : 41559\n",
      "TimeSinceStart : 3.5519144535064697\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.0\n",
      "Eval_StdReturn : 56.780277252197266\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 74.0\n",
      "Eval_AverageEpLen : 154.0\n",
      "Train_AverageReturn : 155.85714721679688\n",
      "Train_StdReturn : 50.65972900390625\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 80.0\n",
      "Train_AverageEpLen : 155.85714285714286\n",
      "Actor Loss : 0.0032333843410015106\n",
      "Baseline Loss : 3196.557861328125\n",
      "Train_EnvstepsSoFar : 42650\n",
      "TimeSinceStart : 3.647778034210205\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 186.3333282470703\n",
      "Eval_StdReturn : 11.897712707519531\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 171.0\n",
      "Eval_AverageEpLen : 186.33333333333334\n",
      "Train_AverageReturn : 177.5\n",
      "Train_StdReturn : 31.420534133911133\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 126.0\n",
      "Train_AverageEpLen : 177.5\n",
      "Actor Loss : -0.016802364960312843\n",
      "Baseline Loss : 3268.318359375\n",
      "Train_EnvstepsSoFar : 43715\n",
      "TimeSinceStart : 3.747189998626709\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 142.3333282470703\n",
      "Eval_StdReturn : 51.48678207397461\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 75.0\n",
      "Eval_AverageEpLen : 142.33333333333334\n",
      "Train_AverageReturn : 188.1666717529297\n",
      "Train_StdReturn : 26.46013832092285\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 129.0\n",
      "Train_AverageEpLen : 188.16666666666666\n",
      "Actor Loss : -0.011850475333631039\n",
      "Baseline Loss : 3559.905517578125\n",
      "Train_EnvstepsSoFar : 44844\n",
      "TimeSinceStart : 3.841355085372925\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 194.6666717529297\n",
      "Eval_StdReturn : 4.109609127044678\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 190.0\n",
      "Eval_AverageEpLen : 194.66666666666666\n",
      "Train_AverageReturn : 175.3333282470703\n",
      "Train_StdReturn : 31.462499618530273\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 125.0\n",
      "Train_AverageEpLen : 175.33333333333334\n",
      "Actor Loss : -0.007963010109961033\n",
      "Baseline Loss : 3146.20556640625\n",
      "Train_EnvstepsSoFar : 45896\n",
      "TimeSinceStart : 3.9372341632843018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 183.0\n",
      "Eval_StdReturn : 7.788880825042725\n",
      "Eval_MaxReturn : 193.0\n",
      "Eval_MinReturn : 174.0\n",
      "Eval_AverageEpLen : 183.0\n",
      "Train_AverageReturn : 169.5\n",
      "Train_StdReturn : 42.80089569091797\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 75.0\n",
      "Train_AverageEpLen : 169.5\n",
      "Actor Loss : 0.007252309005707502\n",
      "Baseline Loss : 3138.4501953125\n",
      "Train_EnvstepsSoFar : 46913\n",
      "TimeSinceStart : 4.029403209686279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 180.3333282470703\n",
      "Eval_StdReturn : 5.792715549468994\n",
      "Eval_MaxReturn : 188.0\n",
      "Eval_MinReturn : 174.0\n",
      "Eval_AverageEpLen : 180.33333333333334\n",
      "Train_AverageReturn : 186.1666717529297\n",
      "Train_StdReturn : 9.702520370483398\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 174.0\n",
      "Train_AverageEpLen : 186.16666666666666\n",
      "Actor Loss : -0.02739107236266136\n",
      "Baseline Loss : 3116.803955078125\n",
      "Train_EnvstepsSoFar : 48030\n",
      "TimeSinceStart : 4.126107215881348\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 188.0\n",
      "Eval_StdReturn : 4.966554641723633\n",
      "Eval_MaxReturn : 195.0\n",
      "Eval_MinReturn : 184.0\n",
      "Eval_AverageEpLen : 188.0\n",
      "Train_AverageReturn : 184.1666717529297\n",
      "Train_StdReturn : 7.470311641693115\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 176.0\n",
      "Train_AverageEpLen : 184.16666666666666\n",
      "Actor Loss : -0.03661293908953667\n",
      "Baseline Loss : 2993.9765625\n",
      "Train_EnvstepsSoFar : 49135\n",
      "TimeSinceStart : 4.223026275634766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 187.6666717529297\n",
      "Eval_StdReturn : 11.671427726745605\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 172.0\n",
      "Eval_AverageEpLen : 187.66666666666666\n",
      "Train_AverageReturn : 156.2857208251953\n",
      "Train_StdReturn : 57.895751953125\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 27.0\n",
      "Train_AverageEpLen : 156.28571428571428\n",
      "Actor Loss : -0.011173443868756294\n",
      "Baseline Loss : 3061.42041015625\n",
      "Train_EnvstepsSoFar : 50229\n",
      "TimeSinceStart : 4.319744110107422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 187.3333282470703\n",
      "Eval_StdReturn : 4.784233093261719\n",
      "Eval_MaxReturn : 194.0\n",
      "Eval_MinReturn : 183.0\n",
      "Eval_AverageEpLen : 187.33333333333334\n",
      "Train_AverageReturn : 180.6666717529297\n",
      "Train_StdReturn : 24.149993896484375\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 132.0\n",
      "Train_AverageEpLen : 180.66666666666666\n",
      "Actor Loss : -0.04101596772670746\n",
      "Baseline Loss : 3087.860107421875\n",
      "Train_EnvstepsSoFar : 51313\n",
      "TimeSinceStart : 4.416123151779175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 197.3333282470703\n",
      "Eval_StdReturn : 3.771235942840576\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 192.0\n",
      "Eval_AverageEpLen : 197.33333333333334\n",
      "Train_AverageReturn : 194.0\n",
      "Train_StdReturn : 6.73300313949585\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 184.0\n",
      "Train_AverageEpLen : 194.0\n",
      "Actor Loss : -0.021243268623948097\n",
      "Baseline Loss : 3386.830322265625\n",
      "Train_EnvstepsSoFar : 52477\n",
      "TimeSinceStart : 4.518278121948242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.012360624969005585\n",
      "Baseline Loss : 3634.710205078125\n",
      "Train_EnvstepsSoFar : 53477\n",
      "TimeSinceStart : 4.6021833419799805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.029310042038559914\n",
      "Baseline Loss : 3610.87109375\n",
      "Train_EnvstepsSoFar : 54477\n",
      "TimeSinceStart : 4.687947034835815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.016731444746255875\n",
      "Baseline Loss : 3586.90869140625\n",
      "Train_EnvstepsSoFar : 55477\n",
      "TimeSinceStart : 4.775031089782715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.005942098796367645\n",
      "Baseline Loss : 3563.750244140625\n",
      "Train_EnvstepsSoFar : 56477\n",
      "TimeSinceStart : 4.8616862297058105\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.002797371009364724\n",
      "Baseline Loss : 3541.87744140625\n",
      "Train_EnvstepsSoFar : 57477\n",
      "TimeSinceStart : 4.944381237030029\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0036505027674138546\n",
      "Baseline Loss : 3521.52978515625\n",
      "Train_EnvstepsSoFar : 58477\n",
      "TimeSinceStart : 5.027453184127808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.007308728061616421\n",
      "Baseline Loss : 3502.72509765625\n",
      "Train_EnvstepsSoFar : 59477\n",
      "TimeSinceStart : 5.11154317855835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.00010686588211683556\n",
      "Baseline Loss : 3485.1259765625\n",
      "Train_EnvstepsSoFar : 60477\n",
      "TimeSinceStart : 5.196451187133789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.00046118354657664895\n",
      "Baseline Loss : 3469.500244140625\n",
      "Train_EnvstepsSoFar : 61477\n",
      "TimeSinceStart : 5.281586170196533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0022344132885336876\n",
      "Baseline Loss : 3425.07080078125\n",
      "Train_EnvstepsSoFar : 62477\n",
      "TimeSinceStart : 5.369678258895874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.001066962257027626\n",
      "Baseline Loss : 3305.033935546875\n",
      "Train_EnvstepsSoFar : 63477\n",
      "TimeSinceStart : 5.452230215072632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.005359940230846405\n",
      "Baseline Loss : 3053.553466796875\n",
      "Train_EnvstepsSoFar : 64477\n",
      "TimeSinceStart : 5.536789417266846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.026699312031269073\n",
      "Baseline Loss : 2927.071044921875\n",
      "Train_EnvstepsSoFar : 65477\n",
      "TimeSinceStart : 5.621430158615112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0005878610536456108\n",
      "Baseline Loss : 2541.759033203125\n",
      "Train_EnvstepsSoFar : 66477\n",
      "TimeSinceStart : 5.704407215118408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.011483865790069103\n",
      "Baseline Loss : 2296.110107421875\n",
      "Train_EnvstepsSoFar : 67477\n",
      "TimeSinceStart : 5.795330047607422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.007416881620883942\n",
      "Baseline Loss : 2166.96337890625\n",
      "Train_EnvstepsSoFar : 68477\n",
      "TimeSinceStart : 5.882633209228516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 176.6666717529297\n",
      "Eval_StdReturn : 32.99831771850586\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 130.0\n",
      "Eval_AverageEpLen : 176.66666666666666\n",
      "Train_AverageReturn : 183.0\n",
      "Train_StdReturn : 38.01315689086914\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 98.0\n",
      "Train_AverageEpLen : 183.0\n",
      "Actor Loss : -0.02440478652715683\n",
      "Baseline Loss : 2095.8271484375\n",
      "Train_EnvstepsSoFar : 69575\n",
      "TimeSinceStart : 5.979394197463989\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.01123179029673338\n",
      "Baseline Loss : 1982.1412353515625\n",
      "Train_EnvstepsSoFar : 70575\n",
      "TimeSinceStart : 6.063848257064819\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0064889187924563885\n",
      "Baseline Loss : 1981.5655517578125\n",
      "Train_EnvstepsSoFar : 71575\n",
      "TimeSinceStart : 6.14866828918457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.010095762088894844\n",
      "Baseline Loss : 1872.4481201171875\n",
      "Train_EnvstepsSoFar : 72575\n",
      "TimeSinceStart : 6.233798265457153\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.002031731652095914\n",
      "Baseline Loss : 1785.07275390625\n",
      "Train_EnvstepsSoFar : 73575\n",
      "TimeSinceStart : 6.321401119232178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.013901479542255402\n",
      "Baseline Loss : 1717.136962890625\n",
      "Train_EnvstepsSoFar : 74575\n",
      "TimeSinceStart : 6.404362440109253\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 184.0\n",
      "Eval_StdReturn : 22.627416610717773\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 152.0\n",
      "Eval_AverageEpLen : 184.0\n",
      "Train_AverageReturn : 178.0\n",
      "Train_StdReturn : 36.482872009277344\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 101.0\n",
      "Train_AverageEpLen : 178.0\n",
      "Actor Loss : -0.022343281656503677\n",
      "Baseline Loss : 1682.117919921875\n",
      "Train_EnvstepsSoFar : 75643\n",
      "TimeSinceStart : 6.499725103378296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.021593978628516197\n",
      "Baseline Loss : 1537.9268798828125\n",
      "Train_EnvstepsSoFar : 76643\n",
      "TimeSinceStart : 6.5840513706207275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 197.3333282470703\n",
      "Train_StdReturn : 5.96284818649292\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 184.0\n",
      "Train_AverageEpLen : 197.33333333333334\n",
      "Actor Loss : -0.015303305350244045\n",
      "Baseline Loss : 1534.0948486328125\n",
      "Train_EnvstepsSoFar : 77827\n",
      "TimeSinceStart : 6.680049180984497\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.01828255131840706\n",
      "Baseline Loss : 1614.5074462890625\n",
      "Train_EnvstepsSoFar : 78827\n",
      "TimeSinceStart : 6.767642259597778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.005809298250824213\n",
      "Baseline Loss : 1413.9119873046875\n",
      "Train_EnvstepsSoFar : 79827\n",
      "TimeSinceStart : 6.852255344390869\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.003857195843011141\n",
      "Baseline Loss : 1402.903564453125\n",
      "Train_EnvstepsSoFar : 80827\n",
      "TimeSinceStart : 6.93693208694458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.005372658837586641\n",
      "Baseline Loss : 1372.178466796875\n",
      "Train_EnvstepsSoFar : 81827\n",
      "TimeSinceStart : 7.021644115447998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0018562956247478724\n",
      "Baseline Loss : 1283.251708984375\n",
      "Train_EnvstepsSoFar : 82827\n",
      "TimeSinceStart : 7.104002237319946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.020541558042168617\n",
      "Baseline Loss : 1272.550537109375\n",
      "Train_EnvstepsSoFar : 83827\n",
      "TimeSinceStart : 7.187857151031494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.005555231124162674\n",
      "Baseline Loss : 1390.643798828125\n",
      "Train_EnvstepsSoFar : 84827\n",
      "TimeSinceStart : 7.271612167358398\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0019164183177053928\n",
      "Baseline Loss : 1238.615966796875\n",
      "Train_EnvstepsSoFar : 85827\n",
      "TimeSinceStart : 7.356554985046387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.005825917236506939\n",
      "Baseline Loss : 1169.1630859375\n",
      "Train_EnvstepsSoFar : 86827\n",
      "TimeSinceStart : 7.4433043003082275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0054130153730511665\n",
      "Baseline Loss : 1406.762939453125\n",
      "Train_EnvstepsSoFar : 87827\n",
      "TimeSinceStart : 7.530526399612427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.004315942991524935\n",
      "Baseline Loss : 1270.46435546875\n",
      "Train_EnvstepsSoFar : 88827\n",
      "TimeSinceStart : 7.616892337799072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0033890714403241873\n",
      "Baseline Loss : 1462.8114013671875\n",
      "Train_EnvstepsSoFar : 89827\n",
      "TimeSinceStart : 7.702920198440552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.006897131446748972\n",
      "Baseline Loss : 1578.164794921875\n",
      "Train_EnvstepsSoFar : 90827\n",
      "TimeSinceStart : 7.787658452987671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.02117253467440605\n",
      "Baseline Loss : 1545.2071533203125\n",
      "Train_EnvstepsSoFar : 91827\n",
      "TimeSinceStart : 7.870152235031128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.011639788746833801\n",
      "Baseline Loss : 2760.730224609375\n",
      "Train_EnvstepsSoFar : 92827\n",
      "TimeSinceStart : 7.954069375991821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.009945906698703766\n",
      "Baseline Loss : 3277.47802734375\n",
      "Train_EnvstepsSoFar : 93827\n",
      "TimeSinceStart : 8.040269136428833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0029237898997962475\n",
      "Baseline Loss : 3397.841552734375\n",
      "Train_EnvstepsSoFar : 94827\n",
      "TimeSinceStart : 8.126904249191284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.004235867410898209\n",
      "Baseline Loss : 3816.05029296875\n",
      "Train_EnvstepsSoFar : 95827\n",
      "TimeSinceStart : 8.213659286499023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0077178762294352055\n",
      "Baseline Loss : 3668.385986328125\n",
      "Train_EnvstepsSoFar : 96827\n",
      "TimeSinceStart : 8.297884225845337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0022756310645490885\n",
      "Baseline Loss : 3755.4453125\n",
      "Train_EnvstepsSoFar : 97827\n",
      "TimeSinceStart : 8.38160514831543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.0070510320365428925\n",
      "Baseline Loss : 3588.8388671875\n",
      "Train_EnvstepsSoFar : 98827\n",
      "TimeSinceStart : 8.466991186141968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 199.0\n",
      "Train_StdReturn : 2.2360680103302\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 194.0\n",
      "Train_AverageEpLen : 199.0\n",
      "Actor Loss : -0.0016443749191239476\n",
      "Baseline Loss : 3408.04052734375\n",
      "Train_EnvstepsSoFar : 100021\n",
      "TimeSinceStart : 8.56228518486023\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : -0.0060699377208948135\n",
      "Baseline Loss : 3292.52294921875\n",
      "Train_EnvstepsSoFar : 101021\n",
      "TimeSinceStart : 8.644836187362671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.004707254935055971\n",
      "Baseline Loss : 3326.72265625\n",
      "Train_EnvstepsSoFar : 102021\n",
      "TimeSinceStart : 8.72955322265625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Actor Loss : 0.002457590540871024\n",
      "Baseline Loss : 3332.712890625\n",
      "Train_EnvstepsSoFar : 103021\n",
      "TimeSinceStart : 8.81379246711731\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize agent\n",
    "agent = PGAgent(\n",
    "    ob_dim,\n",
    "    ac_dim,\n",
    "    discrete,\n",
    "    n_layers=args.n_layers,\n",
    "    layer_size=args.layer_size,\n",
    "    gamma=args.discount,\n",
    "    learning_rate=args.learning_rate,\n",
    "    use_baseline=args.use_baseline,\n",
    "    use_reward_to_go=args.use_reward_to_go,\n",
    "    normalize_advantages=args.normalize_advantages,\n",
    "    baseline_learning_rate=args.baseline_learning_rate,\n",
    "    baseline_gradient_steps=args.baseline_gradient_steps,\n",
    "    gae_lambda=args.gae_lambda,\n",
    ")\n",
    "\n",
    "total_envsteps = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for itr in range(args.n_iter):\n",
    "    print(f\"\\n********** Iteration {itr} ************\")\n",
    "    # TODO: sample `args.batch_size` transitions using utils.sample_trajectories\n",
    "    # make sure to use `max_ep_len`\n",
    "    trajs, envsteps_this_batch = utils.sample_trajectories(\n",
    "        env, agent.actor, args.batch_size, max_ep_len\n",
    "    )\n",
    "    total_envsteps += envsteps_this_batch\n",
    "\n",
    "    # trajs should be a list of dictionaries of NumPy arrays, where each dictionary corresponds to a trajectory.\n",
    "    # this line converts this into a single dictionary of lists of NumPy arrays.\n",
    "    trajs_dict = {k: [traj[k] for traj in trajs] for k in trajs[0]}\n",
    "\n",
    "    # TODO: train the agent using the sampled trajectories and the agent's update function\n",
    "    train_info = agent.update(\n",
    "        trajs_dict[\"observation\"],\n",
    "        trajs_dict[\"action\"], \n",
    "        trajs_dict[\"reward\"],\n",
    "        trajs_dict[\"terminal\"]\n",
    "    )\n",
    "\n",
    "    if itr % args.scalar_log_freq == 0:\n",
    "        # save eval metrics\n",
    "        print(\"\\nCollecting data for eval...\")\n",
    "        eval_trajs, eval_envsteps_this_batch = utils.sample_trajectories(\n",
    "            env, agent.actor, args.eval_batch_size, max_ep_len\n",
    "        )\n",
    "\n",
    "        logs = utils.compute_metrics(trajs, eval_trajs)\n",
    "        # compute additional metrics\n",
    "        logs.update(train_info)\n",
    "        logs[\"Train_EnvstepsSoFar\"] = total_envsteps\n",
    "        logs[\"TimeSinceStart\"] = time.time() - start_time\n",
    "        if itr == 0:\n",
    "            logs[\"Initial_DataCollection_AverageReturn\"] = logs[\n",
    "                \"Train_AverageReturn\"\n",
    "            ]\n",
    "\n",
    "        # perform the logging\n",
    "        for key, value in logs.items():\n",
    "            print(\"{} : {}\".format(key, value))\n",
    "            logger.log_scalar(value, key, itr)\n",
    "        print(\"Done logging...\\n\\n\")\n",
    "\n",
    "        logger.flush()\n",
    "\n",
    "    if args.video_log_freq != -1 and itr % args.video_log_freq == 0:\n",
    "        print(\"\\nCollecting video rollouts...\")\n",
    "        eval_video_trajs = utils.sample_n_trajectories(\n",
    "            env, agent.actor, MAX_NVIDEO, max_ep_len, render=True\n",
    "        )\n",
    "\n",
    "        logger.log_trajs_as_videos(\n",
    "            eval_video_trajs,\n",
    "            itr,\n",
    "            fps=fps,\n",
    "            max_videos_to_save=MAX_NVIDEO,\n",
    "            video_title=\"eval_rollouts\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b150a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
