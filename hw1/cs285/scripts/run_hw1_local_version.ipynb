{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLXw6zd-k3Xd"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook translates run_hw1.py into a more observable ipynb version\n",
    "\n",
    "## Configuration Guide\n",
    "\n",
    "To run different experiments, modify the configuration in Cell 3:\n",
    "\n",
    "### For Behavior Cloning (Problem 1):\n",
    "- Set `do_dagger = False`\n",
    "- Set `n_iter = 1`\n",
    "- Choose your environment: `env_name = 'Ant-v4'` (or Walker2d-v4, HalfCheetah-v4, Hopper-v4)\n",
    "- Set `exp_name = 'bc_ant'` (or bc_walker, bc_cheetah, bc_hopper)\n",
    "\n",
    "### For DAgger (Problem 2):\n",
    "- Set `do_dagger = True`\n",
    "- Set `n_iter = 10` (or more)\n",
    "- Choose your environment\n",
    "- Set `exp_name = 'dagger_ant'` (or dagger_walker, etc.)\n",
    "\n",
    "### Available Environments:\n",
    "- `'Ant-v4'`\n",
    "- `'Walker2d-v4'`\n",
    "- `'HalfCheetah-v4'`\n",
    "- `'Hopper-v4'`\n",
    "\n",
    "### Expert Data Files:\n",
    "- Ant: `'cs285/policies/experts/Ant.pkl'` and `'cs285/expert_data/expert_data_Ant-v4.pkl'`\n",
    "- Walker2d: `'cs285/policies/experts/Walker2d.pkl'` and `'cs285/expert_data/expert_data_Walker2d-v4.pkl'`\n",
    "- HalfCheetah: `'cs285/policies/experts/HalfCheetah.pkl'` and `'cs285/expert_data/expert_data_HalfCheetah-v4.pkl'`\n",
    "- Hopper: `'cs285/policies/experts/Hopper.pkl'` and `'cs285/expert_data/expert_data_Hopper-v4.pkl'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "4HBPnmbIPPyl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/resource_handle_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/resource_handle_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/resource_handle_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/summary_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/summary_pb2.py:38: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/tensorboardX/proto/summary_pb2.py:31: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from cs285.infrastructure import utils\n",
    "from cs285.infrastructure.logger import Logger\n",
    "from cs285.infrastructure.replay_buffer import ReplayBuffer\n",
    "from cs285.policies.MLP_policy import MLPPolicySL\n",
    "from cs285.policies.loaded_gaussian_policy import LoadedGaussianPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rollouts to save as videos to tensorboard\n",
    "MAX_NVIDEO = 2\n",
    "MAX_VIDEO_LEN = 40  # we overwrite this in the code below\n",
    "\n",
    "MJ_ENV_NAMES = [\"Ant-v4\", \"Walker2d-v4\", \"HalfCheetah-v4\", \"Hopper-v4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/xing.zhang/machine-learning/homework_fall2023/hw1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Environment: Ant-v4\n",
      "Experiment: bc_ant\n",
      "DAgger: False\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Set your parameters here\n",
    "class Args:\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    \n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "\n",
    "# Create args object and set all parameters directly\n",
    "args = Args()\n",
    "\n",
    "# Expert data configuration\n",
    "args.expert_policy_file = 'cs285/policies/experts/Ant.pkl'  # relative to where you're running this script from\n",
    "args.expert_data = 'cs285/expert_data/expert_data_Ant-v4.pkl'  # relative to where you're running this script from\n",
    "args.env_name = 'Ant-v4'  # choices: Ant-v4, Walker2d-v4, HalfCheetah-v4, Hopper-v4\n",
    "args.exp_name = 'bc_ant'  # pick an experiment name\n",
    "args.do_dagger = False  # Set to True for DAgger, False for Behavior Cloning\n",
    "args.ep_len = 1000\n",
    "\n",
    "# Training configuration\n",
    "args.num_agent_train_steps_per_iter = 1000  # number of gradient steps for training policy (per iter in n_iter)\n",
    "args.n_iter = 1  # Set to >1 for DAgger\n",
    "\n",
    "# Batch sizes\n",
    "args.batch_size = 1000  # training data collected (in the env) during each iteration\n",
    "args.eval_batch_size = 1000  # eval data collected (in the env) for logging metrics\n",
    "args.train_batch_size = 100  # number of sampled data points to be used per gradient/train step\n",
    "\n",
    "# Network configuration\n",
    "args.n_layers = 2  # depth of policy to be learned\n",
    "args.size = 64  # width of each layer of policy to be learned\n",
    "args.learning_rate = 5e-3  # LR for supervised learning\n",
    "\n",
    "# Logging configuration\n",
    "args.video_log_freq = 5\n",
    "args.scalar_log_freq = 1\n",
    "\n",
    "# GPU configuration\n",
    "args.no_gpu = False\n",
    "args.which_gpu = 0\n",
    "args.max_replay_buffer_size = 1000000\n",
    "args.save_params = False\n",
    "args.seed = 1\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Environment: {args.env_name}\")\n",
    "print(f\"Experiment: {args.exp_name}\")\n",
    "print(f\"DAgger: {args.do_dagger}\")\n",
    "print(f\"Iterations: {args.n_iter}\")\n",
    "\n",
    "# Convert to dictionary for the training function\n",
    "params = vars(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: /Users/xing.zhang/machine-learning/homework_fall2023/hw1/../../data/q1_bc_ant_Ant-v4_2025_09_14_23_06\n",
      "Data path: /Users/xing.zhang/machine-learning/homework_fall2023/hw1/../../data\n"
     ]
    }
   ],
   "source": [
    "# Set up logging directory\n",
    "if args.do_dagger:\n",
    "    logdir_prefix = 'q2_'  # The autograder uses the prefix `q2_`\n",
    "    assert args.n_iter > 1, ('DAgger needs more than 1 iteration (n_iter>1) of training, to iteratively query the expert and train (after 1st warmstarting from behavior cloning).')\n",
    "else:\n",
    "    logdir_prefix = 'q1_'  # The autograder uses the prefix `q1_`\n",
    "    assert args.n_iter == 1, ('Vanilla behavior cloning collects expert data just once (n_iter=1)')\n",
    "\n",
    "# Create data directory\n",
    "data_path = os.path.join(os.getcwd(), '../../data') # NOTE: replaced __file__ with os.getcwd()\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Create log directory\n",
    "logdir = logdir_prefix + args.exp_name + '_' + args.env_name + '_' + time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "logdir = os.path.join(data_path, logdir)\n",
    "args['logdir'] = logdir\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "print(f\"Logging to: {logdir}\")\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce run_training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not detected. Defaulting to CPU.\n",
      "########################\n",
      "logging outputs to  /Users/xing.zhang/machine-learning/homework_fall2023/hw1/../../data/q1_bc_ant_Ant-v4_2025_09_14_23_06\n",
      "########################\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "seed = params['seed']\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "ptu.init_gpu(\n",
    "    use_gpu=not params['no_gpu'],\n",
    "    gpu_id=params['which_gpu']\n",
    ")\n",
    "\n",
    "# Set logger\n",
    "logger = Logger(params['logdir'])\n",
    "log_video = True \n",
    "log_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation and action dims are 27 & 8, respectively\n",
      "Loading expert policy from... cs285/policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/Users/xing.zhang/anaconda3/envs/cs285/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## ENV\n",
    "#############\n",
    "\n",
    "env = gym.make(params['env_name'], render_mode = None)\n",
    "env.reset(seed = seed)\n",
    "\n",
    "vars(env)\n",
    "\n",
    "# Maximum length for episodes\n",
    "params['ep_len'] = params['ep_len'] or env.spec.max_episode_steps\n",
    "MAX_VIDEO_LEN = params['ep_len']\n",
    "\n",
    "# Action and observations\n",
    "assert isinstance(env.action_space, gym.spaces.Box), \"Environment must be continuous\"\n",
    "ob_dim = env.observation_space.shape[0]\n",
    "ac_dim = env.action_space.shape[0]\n",
    "print(f\"observation and action dims are {ob_dim} & {ac_dim}, respectively\")\n",
    "\n",
    "# Simulation timestep, will be used for video saving\n",
    "if 'model' in dir(env):\n",
    "    fps = 1/env.model.opt.timestep\n",
    "else:\n",
    "    fps = env.env.metadata['render_fps']\n",
    "\n",
    "\n",
    "#############\n",
    "## AGENT\n",
    "#############\n",
    "\n",
    "# TODO: Implement missing functions in this class.\n",
    "actor = MLPPolicySL(\n",
    "    ac_dim,\n",
    "    ob_dim,\n",
    "    params['n_layers'],\n",
    "    params['size'],\n",
    "    learning_rate=params['learning_rate'],\n",
    ")\n",
    "\n",
    "# replay buffer\n",
    "replay_buffer = ReplayBuffer(params['max_replay_buffer_size'])\n",
    "\n",
    "#######################\n",
    "## LOAD EXPERT POLICY\n",
    "#######################\n",
    "\n",
    "print('Loading expert policy from...', params['expert_policy_file'])\n",
    "expert_policy = LoadedGaussianPolicy(params['expert_policy_file'])\n",
    "expert_policy.to(ptu.device)\n",
    "print('Done restoring expert policy...')\n",
    "\n",
    "\n",
    "#######################\n",
    "## TRAINING LOOP\n",
    "#######################\n",
    "\n",
    "# init vars at beginning of training\n",
    "total_envsteps = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for itr in range(params['n_iter']):\n",
    "    print(\"\\n\\n********** Iteration %i ************\"%itr)\n",
    "\n",
    "    # decide if videos should be rendered/logged at this iteration\n",
    "    log_video = ((itr % params['video_log_freq'] == 0) and (params['video_log_freq'] != -1))\n",
    "    # decide if metrics should be logged\n",
    "    log_metrics = (itr % params['scalar_log_freq'] == 0)\n",
    "\n",
    "    print(\"\\nCollecting data to be used for training...\")\n",
    "    if itr == 0:\n",
    "        # BC training from expert data.\n",
    "        paths = pickle.load(open(params['expert_data'], 'rb'))\n",
    "        envsteps_this_batch = 0\n",
    "    else:\n",
    "        # DAGGER training from sampled data relabeled by expert\n",
    "        assert params['do_dagger']\n",
    "        # TODO: collect `params['batch_size']` transitions\n",
    "        # HINT: use utils.sample_trajectories\n",
    "        # TODO: implement missing parts of utils.sample_trajectory\n",
    "        paths, envsteps_this_batch = TODO\n",
    "\n",
    "#         # relabel the collected obs with actions from a provided expert policy\n",
    "#         if params['do_dagger']:\n",
    "#             print(\"\\nRelabelling collected observations with labels from an expert policy...\")\n",
    "\n",
    "#             # TODO: relabel collected obsevations (from our policy) with labels from expert policy\n",
    "#             # HINT: query the policy (using the get_action function) with paths[i][\"observation\"]\n",
    "#             # and replace paths[i][\"action\"] with these expert labels\n",
    "#             paths = TODO\n",
    "\n",
    "#     total_envsteps += envsteps_this_batch\n",
    "#     # add collected data to replay buffer\n",
    "#     replay_buffer.add_rollouts(paths)\n",
    "\n",
    "#     # train agent (using sampled data from replay buffer)\n",
    "#     print('\\nTraining agent using sampled data from replay buffer...')\n",
    "#     training_logs = []\n",
    "#     for _ in range(params['num_agent_train_steps_per_iter']):\n",
    "\n",
    "#         # TODO: sample some data from replay_buffer\n",
    "#         # HINT1: how much data = params['train_batch_size']\n",
    "#         # HINT2: use np.random.permutation to sample random indices\n",
    "#         # HINT3: return corresponding data points from each array (i.e., not different indices from each array)\n",
    "#         # for imitation learning, we only need observations and actions.  \n",
    "#         ob_batch, ac_batch = TODO\n",
    "\n",
    "#         # use the sampled data to train an agent\n",
    "#         train_log = actor.update(ob_batch, ac_batch)\n",
    "#         training_logs.append(train_log)\n",
    "\n",
    "#     # log/save\n",
    "#     print('\\nBeginning logging procedure...')\n",
    "#     if log_video:\n",
    "#         # save eval rollouts as videos in tensorboard event file\n",
    "#         print('\\nCollecting video rollouts eval')\n",
    "#         eval_video_paths = utils.sample_n_trajectories(\n",
    "#             env, actor, MAX_NVIDEO, MAX_VIDEO_LEN, True)\n",
    "\n",
    "#         # save videos\n",
    "#         if eval_video_paths is not None:\n",
    "#             logger.log_paths_as_videos(\n",
    "#                 eval_video_paths, itr,\n",
    "#                 fps=fps,\n",
    "#                 max_videos_to_save=MAX_NVIDEO,\n",
    "#                 video_title='eval_rollouts')\n",
    "\n",
    "#     if log_metrics:\n",
    "#         # save eval metrics\n",
    "#         print(\"\\nCollecting data for eval...\")\n",
    "#         eval_paths, eval_envsteps_this_batch = utils.sample_trajectories(\n",
    "#             env, actor, params['eval_batch_size'], params['ep_len'])\n",
    "\n",
    "#         logs = utils.compute_metrics(paths, eval_paths)\n",
    "#         # compute additional metrics\n",
    "#         logs.update(training_logs[-1]) # Only use the last log for now\n",
    "#         logs[\"Train_EnvstepsSoFar\"] = total_envsteps\n",
    "#         logs[\"TimeSinceStart\"] = time.time() - start_time\n",
    "#         if itr == 0:\n",
    "#             logs[\"Initial_DataCollection_AverageReturn\"] = logs[\"Train_AverageReturn\"]\n",
    "\n",
    "#         # perform the logging\n",
    "#         for key, value in logs.items():\n",
    "#             print('{} : {}'.format(key, value))\n",
    "#             logger.log_scalar(value, key, itr)\n",
    "#         print('Done logging...\\n\\n')\n",
    "\n",
    "#         logger.flush()\n",
    "\n",
    "#     if params['save_params']:\n",
    "#         print('\\nSaving agent params')\n",
    "#         actor.save('{}/policy_itr_{}.pt'.format(params['logdir'], itr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is learning notes\n",
    "- Environment\n",
    "- Data: Paths\n",
    "- Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**element 1: observation space**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (27,), float64)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**element 2: action space**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-1.0, 1.0, (8,), float32)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**element 3-5: reward, transition, termination in one step funciton**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_observation\n",
      "[  0.71492013   0.99619409  -0.02240941  -0.04421852  -0.07169297\n",
      "   0.05356397   0.4980131    0.08537807  -0.39134828   0.13119301\n",
      "  -0.45138469   0.02877128   0.48645838  -0.18309693   0.12990067\n",
      "   0.32806806  -0.63449701  -0.44161018  -1.19474622  -0.78696082\n",
      "  12.46278956   0.67844818  -7.34644238   3.57685879 -12.73319225\n",
      "   3.48154923  14.11832739]\n",
      "\n",
      "reward\n",
      "-0.33614342386594953\n",
      "\n",
      "terminated\n",
      "False\n",
      "\n",
      "terminated due to max step hit\n",
      "False\n",
      "\n",
      "info\n",
      "{'reward_forward': -0.09296982462278551, 'reward_ctrl': -1.243173599243164, 'reward_survive': 1.0, 'x_position': -0.0083139772792265, 'y_position': 0.05900007904906596, 'distance_from_origin': 0.059582980338310755, 'x_velocity': -0.09296982462278551, 'y_velocity': 0.1243988547619973, 'forward_reward': -0.09296982462278551}\n",
      "\n",
      "AntEnv  &  gym.envs.mujoco.ant_v4\n",
      "    def step(self, action):\n",
      "        xy_position_before = self.get_body_com(\"torso\")[:2].copy()\n",
      "        self.do_simulation(action, self.frame_skip)\n",
      "        xy_position_after = self.get_body_com(\"torso\")[:2].copy()\n",
      "\n",
      "        xy_velocity = (xy_position_after - xy_position_before) / self.dt\n",
      "        x_velocity, y_velocity = xy_velocity\n",
      "\n",
      "        forward_reward = x_velocity\n",
      "        healthy_reward = self.healthy_reward\n",
      "\n",
      "        rewards = forward_reward + healthy_reward\n",
      "\n",
      "        costs = ctrl_cost = self.control_cost(action)\n",
      "\n",
      "        terminated = self.terminated\n",
      "        observation = self._get_obs()\n",
      "        info = {\n",
      "            \"reward_forward\": forward_reward,\n",
      "            \"reward_ctrl\": -ctrl_cost,\n",
      "            \"reward_survive\": healthy_reward,\n",
      "            \"x_position\": xy_position_after[0],\n",
      "            \"y_position\": xy_position_after[1],\n",
      "            \"distance_from_origin\": np.linalg.norm(xy_position_after, ord=2),\n",
      "            \"x_velocity\": x_velocity,\n",
      "            \"y_velocity\": y_velocity,\n",
      "            \"forward_reward\": forward_reward,\n",
      "        }\n",
      "        if self._use_contact_forces:\n",
      "            contact_cost = self.contact_cost\n",
      "            costs += contact_cost\n",
      "            info[\"reward_ctrl\"] = -contact_cost\n",
      "\n",
      "        reward = rewards - costs\n",
      "\n",
      "        self.renderer.render_step()\n",
      "        return observation, reward, terminated, False, info\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn 1: five key elements of environment: observation space, action space, reward, transition, termination\n",
    "env = gym.make(params['env_name'], render_mode = None, new_step_api=True)\n",
    "\n",
    "display(Markdown(\"**element 1: observation space**\"))\n",
    "print(env.observation_space)\n",
    "\n",
    "\n",
    "display(Markdown(\"**element 2: action space**\"))\n",
    "print(env.action_space)\n",
    "\n",
    "\n",
    "display(Markdown(\"**element 3-5: reward, transition, termination in one step funciton**\"))\n",
    "sample_action = env.action_space.sample()\n",
    "env.reset()\n",
    "results = env.step(sample_action) # results is a tuple\n",
    "result_names = ['next_observation', 'reward', 'terminated', 'terminated due to max step hit', 'info']\n",
    "for result_name, value in zip(result_names, results):\n",
    "    print(result_name)\n",
    "    print(value, end = '\\n\\n')\n",
    "\n",
    "\n",
    "# check the code of step\n",
    "base = env.unwrapped\n",
    "print(base.__class__.__name__, ' & ', base.__module__)\n",
    "#print(inspect.getsourcefile(base.__class__))\n",
    "print(inspect.getsource(base.step))   # shows the actual termination math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of paths is 2\n",
      "each step of the path is a dict with keys: ['observation', 'action', 'reward', 'next_observation', 'terminal']\n",
      "shape of observation: (1000, 27)\n",
      "shape of action: (1000, 8)\n",
      "shape of reward: (1000,)\n",
      "shape of next_observation: (1000, 27)\n",
      "shape of terminal: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# paths is a list\n",
    "print(f\"size of paths is {len(paths)}\")\n",
    "print(f\"each step of the path is a dict with keys: {list(paths[0].keys())}\")\n",
    "for key, item in paths[0].items():\n",
    "    print(f\"shape of {key}: {item.shape}\")\n",
    "\n",
    "# confirm that observation i is next_observation i-1\n",
    "assert np.array_equal(paths[0]['observation'][1:], paths[0]['next_observation'][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor type: <class 'cs285.policies.MLP_policy.MLPPolicySL'>\n",
      "expert policy type: <class 'cs285.policies.loaded_gaussian_policy.LoadedGaussianPolicy'>\n"
     ]
    }
   ],
   "source": [
    "# Policy: actor and expert_policy\n",
    "import torch.nn as nn\n",
    "\n",
    "# both policies are nn.Module\n",
    "assert isinstance(expert_policy, nn.Module)\n",
    "assert isinstance(actor, nn.Module)\n",
    "\n",
    "print(f\"actor type: {type(actor)}\")\n",
    "print(f\"expert policy type: {type(expert_policy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# expert policy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Policy Architecture ===\n",
      "LoadedGaussianPolicy(\n",
      "  (non_lin): Tanh()\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=111, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=8, out_features=64, bias=True)\n",
      ")\n",
      "\n",
      "=== Parameter Counts ===\n",
      "Total parameters:     12,070\n",
      "Trainable parameters: 12,070\n",
      "\n",
      "=== Layer by Layer ===\n",
      "all parameters: [111, 111, 7104, 64, 4096, 64, 512, 8]\n",
      "all trainable parameters: [111, 111, 7104, 64, 4096, 64, 512, 8]\n",
      "obs_norm_mean                  shape=(1, 111) requires_grad=True\n",
      "obs_norm_std                   shape=(1, 111) requires_grad=True\n",
      "hidden_layers.0.weight         shape=(64, 111) requires_grad=True\n",
      "hidden_layers.0.bias           shape=(64,) requires_grad=True\n",
      "hidden_layers.1.weight         shape=(64, 64) requires_grad=True\n",
      "hidden_layers.1.bias           shape=(64,) requires_grad=True\n",
      "output_layer.weight            shape=(8, 64) requires_grad=True\n",
      "output_layer.bias              shape=(8,) requires_grad=True\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# actor"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Policy Architecture ===\n",
      "MLPPolicySL(\n",
      "  (mean_net): Sequential(\n",
      "    (0): Linear(in_features=27, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "=== Parameter Counts ===\n",
      "Total parameters:     6,480\n",
      "Trainable parameters: 6,480\n",
      "\n",
      "=== Layer by Layer ===\n",
      "all parameters: [8, 1728, 64, 4096, 64, 512, 8]\n",
      "all trainable parameters: [8, 1728, 64, 4096, 64, 512, 8]\n",
      "logstd                         shape=(8,) requires_grad=True\n",
      "mean_net.0.weight              shape=(64, 27) requires_grad=True\n",
      "mean_net.0.bias                shape=(64,) requires_grad=True\n",
      "mean_net.2.weight              shape=(64, 64) requires_grad=True\n",
      "mean_net.2.bias                shape=(64,) requires_grad=True\n",
      "mean_net.4.weight              shape=(8, 64) requires_grad=True\n",
      "mean_net.4.bias                shape=(8,) requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "def learn_policy(policy: nn.Module):\n",
    "    # architecture\n",
    "    print(\"=== Policy Architecture ===\")\n",
    "    print(policy)\n",
    "\n",
    "    # total count of parameters\n",
    "    total_params = sum(p.numel() for p in policy.parameters())\n",
    "    trainable_params = sum(p.numel() for p in policy.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\n=== Parameter Counts ===\")\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "\n",
    "    # layers \n",
    "    print(\"\\n=== Layer by Layer ===\")\n",
    "    n_param_expert = [param.numel() for param in policy.parameters()]\n",
    "    n_trainable_param_expert = [param.numel() for param in policy.parameters() if param.requires_grad]\n",
    "    print(f\"all parameters: {n_param_expert}\")\n",
    "    print(f\"all trainable parameters: {n_trainable_param_expert}\")\n",
    "\n",
    "    for name, param in policy.named_parameters():\n",
    "        print(f\"{name:30} shape={tuple(param.shape)} requires_grad={param.requires_grad}\")\n",
    "\n",
    "display(Markdown(\"# expert policy\"))\n",
    "learn_policy(expert_policy)\n",
    "display(Markdown(\"# actor\"))\n",
    "learn_policy(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "mLXw6zd-k3Xd",
    "UunygyDXrx7k"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
